{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[svm]201914177윤영진 (1).ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"qbltlYE_x4ZX"},"source":["<h2>개인 구글 드라이브와 colab 연동</h2>"]},{"cell_type":"code","metadata":{"id":"j6q5Z9pNGA0M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634484963773,"user_tz":-540,"elapsed":24590,"user":{"displayName":"윤영진","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04470020603917047117"}},"outputId":"8607f03b-518e-4b27-f270-26a07f49f310"},"source":["from google.colab import drive\n","drive.mount(\"/gdrive\", force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}]},{"cell_type":"markdown","metadata":{"id":"PlXCd1UpzDLL"},"source":["<h2>\"SMSSpamCollection\" 데이터를 읽고 문장과 정답을 분리하여 각 리스트에 저장</h2>\n","\n","<pre>\n","<b>1. 데이터의 형태(SMSSpamCollection)</b>\n","  라벨(스팸 또는 햄) \\t(tab) 문장 \n","  \n","  위와 같은 형태로 저장되어 있음\n","  \n","  예시)\n","    ham\\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n","    spam\\tCustomer service annoncement. You have a New Years delivery waiting for you. Please call 07046744435 now to arrange delivery\n","    ...\n","  \n","  따라서 입력 데이터를 읽고 \\t을 기준으로 입력 문장을 분리한 후에 문장과 라벨을 각각 x_data, y_data 리스트에 저장\n","  \n","<b>2. 입력 데이터 전체를 사용하지 않고 100개만 추출해서 사용</b>\n","\n","<b>3. x_data, y_data 형태</b>\n","  x_data = [ 문장1, 문장2, 문장3, ... 문장100]\n","  y_data = [ 문장1의 라벨, 문장2의 라벨, 문장3의 라벨, ... 문장100의 라벨]\n","</pre>"]},{"cell_type":"code","metadata":{"id":"gUuFzwfHGFrq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634484969278,"user_tz":-540,"elapsed":2550,"user":{"displayName":"윤영진","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04470020603917047117"}},"outputId":"f0c09751-4a4d-4f5f-e007-055c1ab9d9da"},"source":["import numpy as np\n","import nltk\n","\n","\n","file_path = \"/gdrive/MyDrive/colab/svm/SMSSpamCollection.dat\"\n","\n","# 파일 읽기\n","x_data, y_data = [], []\n","with open(file_path,'r',encoding='utf8') as inFile:\n","  lines = inFile.readlines()\n","  \n","lines = lines[:100]  \n","\n","for line in lines:\n","  line = line.strip().split('\\t')\n","  sentence, label = line[1], line[0]\n","  x_data.append(sentence)\n","  y_data.append(label)\n","  \n","print(\"x_data의 개수 : \" + str(len(x_data)))\n","print(\"y_data의 개수 : \" + str(len(y_data)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["x_data의 개수 : 100\n","y_data의 개수 : 100\n"]}]},{"cell_type":"markdown","metadata":{"id":"3R9OE2Cp1jRX"},"source":["<h2>Tokenizer 라이브러리를 사용하여 입력 문장을 index로 치환</h2>\n","\n","<pre>\n","<b>1. tokenizer.fit_on_texts(data) 함수를 이용하여 각 단어를 index로 치환하기 위한 딕셔너리 생성</b>\n","   생성된 딕셔너리는 tokenizer 객체 안에 저장됨\n","  \n","  tokenizer.fit_on_texts(data)\n","  args\n","    data : 문자열 element를 가지고 있는 리스트\n","  return\n","    X\n","    \n","  딕셔너리 예시)\n","    {'to': 1, 'i': 2, 'you': 3, 'a': 4, 'the': 5, 'and': 6, 'for': 7 ... }\n","    \n","<b>2. tokenizer.texts_to_sequences(data) 함수를 이용하여 문장 안에 있는 단어들을 index로 치환</b>\n","\n","  tokenizer.texts_to_sequence(data)\n","  args\n","    data : 문자열 element를 가지고 있는 리스트\n","  return : \n","    indexing 된 리스트\n","    \n","  indexing 예시)\n","    x_data indexing 하기 전 : Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n","    x_data indexing 하기 후 : [38, 93, 239, 240, 241, 242, 53, 11, 243, 72, 94, 244, 245, 126, 246, 247, 73, 74, 248, 127]\n","    y_data indexing 하기 전 : ham\n","    y_data indexing 하기 후 : 1\n","</pre>"]},{"cell_type":"code","metadata":{"id":"YEagO2Q0GOBM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634485036565,"user_tz":-540,"elapsed":406,"user":{"displayName":"윤영진","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04470020603917047117"}},"outputId":"d989ddff-1802-4612-932a-4e9fbc5d2074"},"source":["from keras.preprocessing.text import Tokenizer\n","from nltk.corpus import stopwords \n","nltk.download('stopwords')\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')\n","\n","tokenizer = Tokenizer()\n","\n","# spam, ham 라벨을 대응하는 index로 치환하기위한 딕셔너리\n","label2index_dict = {'spam':0, 'ham':1}\n","\n","# indexing 한 데이터를 넣을 리스트 선언\n","indexing_x_data, indexing_y_data = [], []\n","\n","for label in y_data:\n","  indexing_y_data.append(label2index_dict[label])\n","\n","stop_words = stopwords.words('english')\n","\n","print(stopwords.words('english'))\n","\n","\n","\n","i = 0\n","for temp in x_data :\n","  \n","  temp = temp.replace('‘','\\'') # don't를 do와 n't로 나누기 위해 특수문자 '‘' 를 '\\''로 변경\n","  result = \"\"\n","  word_tokens = word_tokenize(temp.lower()) # 불용어 리스트에 소문자만 있으므로 문장을 모두 소문자로 변경 후 토크나이징\n","  \n","  for w in word_tokens :\n","    \n","    if w not in stop_words :\n","      \n","      result = result + w + \" \"\n","    \n","  x_data[i] = result\n","  i = i +1\n","\n","# x_data를 사용하여 딕셔너리 생성\n","tokenizer.fit_on_texts(x_data)    \n","# word_index = tokenizer.word_index\n","# print(word_index)               \n","\n","# x_data에 있는 각 문장의 단어들을 대응하는 index로 치환하고 그 결과값을 indexing_x_data에 저장\n","indexing_x_data = tokenizer.texts_to_sequences(x_data)    \n","\n","print(\"x_data indexing 하기 전 : \" + str(x_data[0]))\n","print(\"x_data indexing 하기 후 : \" + str(indexing_x_data[0]))\n","print(\"y_data indexing 하기 전 : \" + str(y_data[0]))\n","print(\"y_data indexing 하기 후 : \" + str(indexing_y_data[0]))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n","x_data indexing 하기 전 : go jurong point , crazy.. available bugis n great world la e buffet ... cine got amore wat ... \n","x_data indexing 하기 후 : [10, 173, 174, 175, 176, 177, 30, 44, 178, 179, 69, 180, 181, 17, 182, 70]\n","y_data indexing 하기 전 : ham\n","y_data indexing 하기 후 : 1\n"]}]},{"cell_type":"markdown","metadata":{"id":"nN5GGyQI_IpT"},"source":["<h2>SVM 모델 학습</h2>\n","\n","<pre>\n","<b>1. 데이터의 문장 길이를 고정된 길이로 변환</b>\n","  예제 코드에서는 60으로 맞추어 변환\n","  \n","  문장 길이가 60 초과인 경우 뒷 부분 제거\n","  문장 길이가 60 미만인 경우 나머지 부분을 0으로 채움\n","  \n","  예시)\n","    문장 길이를 5로 맞추고자 할 경우\n","    \n","    문장 길이가 5보다 큰 경우\n","    문장 : 나는 어제 집에서 저녁으로 김치찌개를 먹었다, indexing_문장 : 38, 93, 239, 240, 241, 242\n","    38, 93, 239, 240, 241, 242 -> 38, 93, 239, 240, 241\n","    \n","    문장 길이가 5보다 작은 경우\n","    문장 : 나는 김치찌개를 좋아해, indexing_문장 : 74, 248, 127\n","    74, 248, 127 -> 74, 248, 127, 0, 0\n","    \n","<b>2. 입력 데이터를 9 대 1 비율로 나누어 학습, 평가에 사용</b>\n","  train_x = [ 문장1, 문장2, 문장3, ... 문장90]\n","  train_y = [ 문장1의 라벨, 문장2의 라벨, 문장3의 라벨, ... 문장90의 라벨]\n","  test_x = [ 문장91, 문장92, 문장93, ... 문장100]\n","  test_y = [ 문장91의 라벨, 문장92의 라벨, 문장93의 라벨, ... 문장100의 라벨]\n","  \n","<b>3. svm.fit(x, y) 함수를 사용하여 SVM 모델 학습</b>\n","  svm.fit(x, y)\n","  args\n","    x : indexing 된 문장들이 있는 리스트\n","    y : x의 각 문장에 대응하는 라벨이 있는 리스트\n","  return : \n","    X\n","</pre>"]},{"cell_type":"code","metadata":{"id":"RYNBrDnzGO-5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634485040370,"user_tz":-540,"elapsed":395,"user":{"displayName":"윤영진","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04470020603917047117"}},"outputId":"88188627-753f-4525-dc3d-1bc9909551bc"},"source":["from sklearn.svm import SVC\n","\n","# 문장의 길이를 max_length으로 맞춰 변환\n","max_length = 60\n","for index in range(len(indexing_x_data)):\n","  length = len(indexing_x_data[index])\n","  \n","  if(length > max_length):\n","    indexing_x_data[index] = indexing_x_data[index][:max_length]\n","  elif(length < max_length):\n","    indexing_x_data[index] = indexing_x_data[index] + [0]*(max_length-length)\n","    \n","    \n","# 전체 데이터를 9:1의 비율로 나누어 학습 및 평가 데이터로 사용\n","number_of_train = int(len(indexing_x_data)*0.9)\n","\n","train_x = indexing_x_data[:number_of_train]\n","train_y = indexing_y_data[:number_of_train]\n","test_x = indexing_x_data[number_of_train:]\n","test_y = indexing_y_data[number_of_train:]\n","\n","print(\"train_x의 개수 : \" + str(len(train_x)))\n","print(\"train_y의 개수 : \" + str(len(train_y)))\n","print(\"test_x의 개수 : \" + str(len(test_x)))\n","print(\"test_y의 개수 : \" + str(len(test_y)))\n","\n","svm = SVC(kernel='linear', C=1e10)\n","svm.fit(train_x, train_y)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train_x의 개수 : 90\n","train_y의 개수 : 90\n","test_x의 개수 : 10\n","test_y의 개수 : 10\n"]},{"output_type":"execute_result","data":{"text/plain":["SVC(C=10000000000.0, break_ties=False, cache_size=200, class_weight=None,\n","    coef0=0.0, decision_function_shape='ovr', degree=3, gamma='scale',\n","    kernel='linear', max_iter=-1, probability=False, random_state=None,\n","    shrinking=True, tol=0.001, verbose=False)"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"l8Ez7hULOckE"},"source":["<h2>SVM 모델을 이용한 평가</h2>\n","\n","<pre>\n","<b>1. svm.predict(data) 함수를 사용하여 SVM 모델을 이용하여 평가</b>\n","  \n","  svm.predict(data)\n","  args\n","    data : indexing 된 문장들이 있는 리스트\n","  return : \n","    입력 문장들에 대한 모델의 출력 라벨 리스트\n","    \n","<b>2. 성능 측정</b>\n","  정답 라벨과 모델의 출력 라벨을 비교하여 성능 측정\n","  \n","<b>3. tokenizer.sequences_to_texts(data) 함수를 이용하여 indexing 된 데이터를 단어로 치환</b>\n","\n","  tokenizer.sequences_to_texts(data)\n","  args\n","    data : indexing 된 리스트\n","  return : \n","    단어로 치환된 리스트\n","    \n","  예시)\n","    [38, 93, 239, 240, 241, 242, 53, 11, 243, 72, 94, 244, 245, 126, 246, 247, 73, 74, 248, 127] -> Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n","    \n","<b>4. 입력 문장에 대한 모델의 출력과 정답 출력</b>\n","\n","</pre>"]},{"cell_type":"code","metadata":{"id":"gONe3GnfGQcu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634485045184,"user_tz":-540,"elapsed":397,"user":{"displayName":"윤영진","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04470020603917047117"}},"outputId":"e9d9aaf9-df06-48a9-a7b6-1f38b068274f"},"source":["predict = svm.predict(test_x)\n","\n","correct_count = 0\n","for index in range(len(predict)):\n","  if(test_y[index] == predict[index]):\n","    correct_count += 1\n","    \n","accuracy = 100.0*correct_count/len(test_y)\n","\n","\n","print(\"Accuracy: \" + str(accuracy))\n","\n","index2label = {0:\"spam\", 1:\"ham\"}\n","\n","test_x_word = tokenizer.sequences_to_texts(test_x)\n","\n","for index in range(len(test_x_word)):\n","  print()\n","  print(\"문장 : \", test_x_word[index])\n","  print(\"정답 : \", index2label[test_y[index]])\n","  print(\"모델 출력 : \", index2label[predict[index]])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 60.0\n","\n","문장 :  yeah n't stand close tho 'll catch something\n","정답 :  ham\n","모델 출력 :  ham\n","\n","문장 :  sorry pain ok meet another night spent late afternoon casualty means n't done stuff42moro includes time sheets sorry\n","정답 :  ham\n","모델 출력 :  spam\n","\n","문장 :  smile pleasure smile pain smile trouble pours like rain smile sum1 hurts u smile becoz someone still loves see u smiling\n","정답 :  ham\n","모델 출력 :  spam\n","\n","문장 :  please call customer service representative 0800 169 6031 10am 9pm guaranteed ￡1000 cash ￡5000 prize\n","정답 :  spam\n","모델 출력 :  spam\n","\n","문장 :  havent planning buy later check already lido got 530 show e afternoon u finish work already\n","정답 :  ham\n","모델 출력 :  spam\n","\n","문장 :  free ringtone waiting collected simply text password mix '' 85069 verify get usher britney fml po box 5249 mk17 92h 450ppw 16\n","정답 :  spam\n","모델 출력 :  spam\n","\n","문장 :  watching telugu movie wat abt u\n","정답 :  ham\n","모델 출력 :  ham\n","\n","문장 :  see finish loads loans pay\n","정답 :  ham\n","모델 출력 :  ham\n","\n","문장 :  hi wk ok hols yes bit run forgot hairdressers appointment four need get home n shower beforehand cause prob u ''\n","정답 :  ham\n","모델 출력 :  spam\n","\n","문장 :  see cup coffee animation\n","정답 :  ham\n","모델 출력 :  ham\n"]}]}]}